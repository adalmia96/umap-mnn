{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install Packages\n",
    "\n",
    "!pip3 install mnist\n",
    "!pip3 install keras\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import scipy\n",
    "import numba\n",
    "import pickle\n",
    "import gzip\n",
    "\n",
    "from scipy import spatial\n",
    "from scipy import optimize\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import SpectralEmbedding\n",
    "\n",
    "\n",
    "\n",
    "from sklearn import datasets,metrics,preprocessing\n",
    "from sklearn.datasets import fetch_20newsgroups, fetch_openml\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, ParameterGrid\n",
    "#from keras.datasets import cifar10\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib import cm\n",
    "\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import mnist\n",
    "from sklearn import datasets,metrics\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pynndescent import NNDescent\n",
    "#from umap import UMAP\n",
    "\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load full MNIST dataset\n",
    "MNIST = fetch_openml('mnist_784', version=1) \n",
    "MNIST_X_train = MNIST.data #np.vstack((mnist.train_images(), mnist.test_images()))\n",
    "MNIST_n_samples = len(MNIST_X_train)\n",
    "MNIST_y_train = MNIST.target.astype(int) #np.vstack((mnist.train_labels().reshape(-1, 1), mnist.test_labels().reshape(-1, 1)))\n",
    "\n",
    "\n",
    "#Load 20 NewsGroup\n",
    "vectorizer = CountVectorizer(min_df=5, stop_words='english') \n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(subset='all')\n",
    "newsgroups_X_train = vectorizer.fit_transform(newsgroups_train.data).toarray()\n",
    "newsgroups_n_train = len(newsgroups_X_train)\n",
    "newsgroups_y_train = newsgroups_train.target\n",
    "\n",
    "#Load FMNIST\n",
    "fmnist = fetch_openml('Fashion-MNIST', version=1)\n",
    "#print(fmnist)\n",
    "f_mnist_X = fmnist.data\n",
    "f_mnist_n = len(f_mnist_X)\n",
    "f_mnist_y = fmnist.target.astype(int)\n",
    "\n",
    "#Load AG. Files downloaded at http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles.html\n",
    "AG_train_data = pd.read_csv('data/AG/train.csv')\n",
    "AG_test_data = pd.read_csv('data/AG/test.csv')\n",
    "AG_data = pd.concat([AG_train_data, AG_test_data], axis=0)\n",
    "AG_data['text'] = AG_data['Title'] + ' ' + AG_data['Description'] \n",
    "AG_data = AG_data.drop(columns=['Title', 'Description'])\n",
    "\n",
    "AG_X_train =  AG_data['text'].to_numpy()\n",
    "AG_y_train =  AG_data['Class Index'].to_numpy()\n",
    "AG_X_train = vectorizer.fit_transform(AG_X_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = newsgroups_X_train\n",
    "y = newsgroups_y_train\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random State\n",
    "from sklearn.utils import check_random_state\n",
    "random_state = check_random_state(0)\n",
    "\n",
    "# Distance Metric to Use\n",
    "metric = 'jaccard'\n",
    "\n",
    "# number of neighbors for computing k-neighbor graph\n",
    "n_neighbors = 30\n",
    "\n",
    "# new number of neighbors to search for\n",
    "new_n_neighbors = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate a,b hyperparams given the min_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap.umap_ import find_ab_params\n",
    "\n",
    "min_dist = 0.1\n",
    "a, b = find_ab_params(1.0, min_dist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the weighted nearest neighbor graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap.umap_ import nearest_neighbors\n",
    "\n",
    "knn_indices, knn_dists, knn_search_index = nearest_neighbors(\n",
    "    X,\n",
    "    n_neighbors=n_neighbors,\n",
    "    metric = metric,\n",
    "    metric_kwds = {},\n",
    "    angular=False,\n",
    "    random_state = random_state,\n",
    "    low_memory=True,\n",
    "    use_pynndescent=True,\n",
    "    n_jobs=1,\n",
    "    verbose=True,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate min spanning tree\n",
    "\n",
    "def min_spanning_tree(knn_indices, knn_dists, n_neighbors, threshold):\n",
    "  \n",
    "  rows = np.zeros(knn_indices.shape[0] * n_neighbors, dtype=np.int32)\n",
    "  cols = np.zeros(knn_indices.shape[0] * n_neighbors, dtype=np.int32)\n",
    "  vals = np.zeros(knn_indices.shape[0] * n_neighbors, dtype=np.float32)\n",
    "  \n",
    "  pos = 0\n",
    "  for i, indices in enumerate(knn_indices):\n",
    "    for j, index in enumerate(indices[:threshold]):\n",
    "      if index == -1:\n",
    "        continue\n",
    "      rows[pos] = i \n",
    "      cols[pos] = index\n",
    "      vals[pos] = knn_dists[i][j]\n",
    "      pos += 1\n",
    "  \n",
    "  matrix = scipy.sparse.csr_matrix((vals, (rows, cols)), shape=(knn_indices.shape[0], knn_indices.shape[0]))\n",
    "  Tcsr = scipy.sparse.csgraph.minimum_spanning_tree(matrix)\n",
    "  \n",
    "  Tcsr = scipy.sparse.coo_matrix(Tcsr)\n",
    "  weights_tuples = zip(Tcsr.row, Tcsr.col, Tcsr.data)\n",
    "  \n",
    "\n",
    "  sorted_weights_tuples = sorted(weights_tuples, key=lambda tup: tup[2])\n",
    "  \n",
    "  return sorted_weights_tuples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import heapq\n",
    "\n",
    "def create_connected_graph(mutual_nn, total_mutual_nn, knn_indices, knn_dists, n_neighbors, connectivity):\n",
    "  connected_mnn = copy.deepcopy(mutual_nn)\n",
    "  \n",
    "  if connectivity == \"nearest\":\n",
    "    for i in range(len(knn_indices)): \n",
    "      if len(mutual_nn[i]) == 0:\n",
    "        first_nn = knn_indices[i][1]\n",
    "        if first_nn != -1:\n",
    "          connected_mnn[i].add(first_nn) \n",
    "          connected_mnn[first_nn].add(i) \n",
    "          total_mutual_nn += 1\n",
    "    return connected_mnn\n",
    "\n",
    "            \n",
    "      \n",
    "  #Create graph for mutual NN\n",
    "  rows = np.zeros(total_mutual_nn, dtype=np.int32)\n",
    "  cols = np.zeros(total_mutual_nn, dtype=np.int32)\n",
    "  vals = np.zeros(total_mutual_nn, dtype=np.float32)\n",
    "  pos = 0\n",
    "  for i in connected_mnn:\n",
    "    for j in connected_mnn[i]:\n",
    "      rows[pos] = i \n",
    "      cols[pos] = j\n",
    "      vals[pos] = 1\n",
    "      pos += 1\n",
    "  graph = scipy.sparse.csr_matrix((vals, (rows, cols)), shape=(knn_indices.shape[0], knn_indices.shape[0]))\n",
    "  \n",
    "  \n",
    "  #Find number of connected components\n",
    "  n_components, labels = scipy.sparse.csgraph.connected_components(csgraph=graph, directed=True, return_labels=True, connection= 'strong')\n",
    "  print(n_components)\n",
    "  label_mapping = {i:[] for i in range(n_components)}\n",
    "\n",
    "  for index, component in enumerate(labels):\n",
    "    label_mapping[component].append(index)\n",
    "\n",
    "\n",
    "\n",
    "  #Find the min spanning tree with KNN\n",
    "  sorted_weights_tuples = min_spanning_tree(knn_indices, knn_dists, n_neighbors, n_neighbors)\n",
    "  \n",
    "\n",
    "  #Add edges until graph is connected\n",
    "  for pos,(i,j,v) in enumerate(sorted_weights_tuples):\n",
    "\n",
    "    if connectivity == \"full_tree\":\n",
    "      connected_mnn[i].add(j)\n",
    "      connected_mnn[j].add(i) \n",
    "      \n",
    "      \n",
    "    elif connectivity == \"min_tree\" and labels[i] != labels[j]:\n",
    "      if len(label_mapping[labels[i]]) < len(label_mapping[labels[j]]):\n",
    "        i, j = j, i\n",
    "        \n",
    "      connected_mnn[i].add(j)\n",
    "      connected_mnn[j].add(i)\n",
    "      j_pos = label_mapping[labels[j]]\n",
    "      labels[j_pos] = labels[i]\n",
    "      label_mapping[labels[i]].extend(j_pos)\n",
    "\n",
    "  return connected_mnn  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#Search to find adjacent neighbors \n",
    "def find_new_nn(knn_indices, knn_dists, knn_indices_pos, connected_mnn, n_neighbors_max, verbose=False):\n",
    "  \n",
    "  new_knn_dists= [] \n",
    "  new_knn_indices = []\n",
    "  \n",
    "  for i in range(len(knn_indices)): \n",
    "    #print(i)\n",
    "    min_distances = []\n",
    "    min_indices = []\n",
    "    #Initialize vars\n",
    "    heap = [(0,0,i)]\n",
    "    mapping = {}\n",
    "          \n",
    "    seen = set()\n",
    "    heapq.heapify(heap) \n",
    "    while(len(min_distances) < n_neighbors_max and len(heap) >0):\n",
    "      dist, hop, nn = heapq.heappop(heap)\n",
    "      if nn == -1:\n",
    "        continue\n",
    "      #For adjacent, only considering one hop away\n",
    "      if nn not in seen and hop <= 1:\n",
    "        min_distances.append(dist)\n",
    "        min_indices.append(nn)\n",
    "        seen.add(nn)\n",
    "        neighbor = connected_mnn[nn]\n",
    "        \n",
    "        for nn_nn in neighbor:\n",
    "          if nn_nn not in seen and hop <= 0:\n",
    "            distance = 0\n",
    "            if nn_nn in knn_indices_pos[nn]:\n",
    "              pos = knn_indices_pos[nn][nn_nn]\n",
    "              distance = knn_dists[nn][pos] \n",
    "            else:\n",
    "              pos = knn_indices_pos[nn_nn][nn]\n",
    "              distance = knn_dists[nn_nn][pos] \n",
    "            distance += dist\n",
    "            \n",
    "            if nn_nn not in mapping:\n",
    "              mapping[nn_nn] = distance\n",
    "              heapq.heappush(heap, (distance, hop+1, nn_nn))\n",
    "            elif mapping[nn_nn] > distance:\n",
    "              mapping[nn_nn] = distance\n",
    "              heapq.heappush(heap, (distance, hop+1, nn_nn))\n",
    "    \n",
    "    if len(min_distances) < n_neighbors_max:\n",
    "      for j in range(n_neighbors_max-len(min_distances)):\n",
    "        min_indices.append(-1)\n",
    "        min_distances.append(np.inf)\n",
    "    \n",
    "    new_knn_dists.append(min_distances)\n",
    "    new_knn_indices.append(min_indices)\n",
    "    \n",
    "    if verbose and i % int(len(knn_dists) / 10) == 0:\n",
    "      print(\"\\tcompleted \", i, \" / \", len(knn_dists), \"epochs\")\n",
    "  return new_knn_dists, new_knn_indices\n",
    "\n",
    "\n",
    "\n",
    "#Calculate the connected mutual nn graph\n",
    "def mutual_nn_nearest(knn_indices, knn_dists, n_neighbors, n_neighbors_max, connectivity=\"min_tree\", verbose=False):\n",
    "  mutual_nn = {}\n",
    "  nearest_n= {}\n",
    "\n",
    "  knn_indices_pos = [None] * len(knn_indices)\n",
    "\n",
    "  total = 0\n",
    "  \n",
    "  for i, top_vals in enumerate(knn_indices):\n",
    "    nearest_n[i] = set(top_vals)\n",
    "    knn_indices_pos[i] = {}\n",
    "    for pos, nn in enumerate(top_vals):\n",
    "      knn_indices_pos[i][nn] = pos\n",
    "  \n",
    "  total_mutual_nn = 0\n",
    "  for i, top_vals in enumerate(knn_indices):\n",
    "    mutual_nn[i] = set()\n",
    "    for ind, nn in enumerate(top_vals):\n",
    "      if nn != -1 and (i in nearest_n[nn] and i != nn):\n",
    "        mutual_nn[i].add(nn)\n",
    "        total_mutual_nn += 1\n",
    "\n",
    "  \n",
    "  connected_mnn = create_connected_graph(mutual_nn, total_mutual_nn, knn_indices, knn_dists, n_neighbors, connectivity )\n",
    "  new_knn_dists, new_knn_indices = find_new_nn(knn_indices, knn_dists, knn_indices_pos, connected_mnn, n_neighbors_max, verbose)\n",
    "\n",
    "  \n",
    "  return connected_mnn, mutual_nn, np.array(new_knn_indices), np.array(new_knn_dists)  \n",
    "\n",
    "    \n",
    "connected_mnn,mutual_nn, new_knn_indices, new_knn_dists  = mutual_nn_nearest(knn_indices, knn_dists, n_neighbors, new_n_neighbors, connectivity= \"nearest\" , verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the Fuzzy Simplicial Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap.umap_ import compute_membership_strengths\n",
    "\n",
    "INT32_MIN = np.iinfo(np.int32).min + 1\n",
    "INT32_MAX = np.iinfo(np.int32).max - 1\n",
    "\n",
    "SMOOTH_K_TOLERANCE = 1e-5\n",
    "MIN_K_DIST_SCALE = 1e-3\n",
    "NPY_INFINITY = np.inf\n",
    "\n",
    "@numba.njit(\n",
    "    locals={\n",
    "        \"psum\": numba.types.float32,\n",
    "        \"lo\": numba.types.float32,\n",
    "        \"mid\": numba.types.float32,\n",
    "        \"hi\": numba.types.float32,\n",
    "    },\n",
    "    fastmath=True,\n",
    ")  \n",
    "\n",
    "#Modified to use a variable k as explained paper\n",
    "def smooth_knn_dist(distances, k, n_iter=64, local_connectivity=1.0, bandwidth=1.0):\n",
    "\n",
    "    #target = np.log2(k) * bandwidth\n",
    "    rho = np.zeros(distances.shape[0], dtype=np.float32)\n",
    "    result = np.zeros(distances.shape[0], dtype=np.float32)\n",
    "\n",
    "    mean_distances = np.mean(distances)\n",
    "\n",
    "    for i in range(distances.shape[0]):\n",
    "\n",
    "        \n",
    "        lo = 0.0\n",
    "        hi = NPY_INFINITY\n",
    "        mid = 1.0\n",
    "\n",
    "        # TODO: This is very inefficient, but will do for now. FIXME\n",
    "        ith_distances = distances[i]\n",
    "        non_zero_dists = ith_distances[ith_distances > 0.0]\n",
    "        \n",
    "        # New k\n",
    "        non_inf_dists = ith_distances[ith_distances != np.inf]\n",
    "        target = np.log2(len(non_inf_dists)) * bandwidth\n",
    "        #print(target)\n",
    "        \n",
    "        \n",
    "        if non_zero_dists.shape[0] >= local_connectivity:\n",
    "            index = int(np.floor(local_connectivity))\n",
    "            interpolation = local_connectivity - index\n",
    "            if index > 0:\n",
    "                rho[i] = non_zero_dists[index - 1]\n",
    "                if interpolation > SMOOTH_K_TOLERANCE:\n",
    "                    rho[i] += interpolation * (\n",
    "                        non_zero_dists[index] - non_zero_dists[index - 1]\n",
    "                    )\n",
    "            else:\n",
    "                rho[i] = interpolation * non_zero_dists[0]\n",
    "        elif non_zero_dists.shape[0] > 0:\n",
    "            rho[i] = np.max(non_zero_dists)\n",
    "\n",
    "        for n in range(n_iter):\n",
    "\n",
    "            psum = 0.0\n",
    "            for j in range(1, distances.shape[1]):\n",
    "                d = distances[i, j] - rho[i]\n",
    "                if d > 0:\n",
    "                    psum += np.exp(-(d / mid))\n",
    "                else:\n",
    "                    psum += 1.0\n",
    "\n",
    "            if np.fabs(psum - target) < SMOOTH_K_TOLERANCE:\n",
    "                break\n",
    "\n",
    "            if psum > target:\n",
    "                hi = mid\n",
    "                mid = (lo + hi) / 2.0\n",
    "            else:\n",
    "                lo = mid\n",
    "                if hi == NPY_INFINITY:\n",
    "                    mid *= 2\n",
    "                else:\n",
    "                    mid = (lo + hi) / 2.0\n",
    "\n",
    "        result[i] = mid\n",
    "\n",
    "        # TODO: This is very inefficient, but will do for now. FIXME\n",
    "        if rho[i] > 0.0:\n",
    "            mean_ith_distances = np.mean(ith_distances)\n",
    "            if result[i] < MIN_K_DIST_SCALE * mean_ith_distances:\n",
    "                result[i] = MIN_K_DIST_SCALE * mean_ith_distances\n",
    "        else:\n",
    "            if result[i] < MIN_K_DIST_SCALE * mean_distances:\n",
    "                result[i] = MIN_K_DIST_SCALE * mean_distances\n",
    "\n",
    "    return result, rho\n",
    "\n",
    "#Calculate fuzzy_simplicial_set with new variable k\n",
    "def fuzzy_simplicial_set(\n",
    "    X,\n",
    "    n_neighbors,\n",
    "    random_state,\n",
    "    metric,\n",
    "    metric_kwds={},\n",
    "    knn_indices=None,\n",
    "    knn_dists=None,\n",
    "    angular=False,\n",
    "    set_op_mix_ratio=1.0,\n",
    "    local_connectivity=1.0,\n",
    "    apply_set_operations=True,\n",
    "    verbose=False,\n",
    "    return_dists=None,\n",
    "):\n",
    "    if knn_indices is None or knn_dists is None:\n",
    "        knn_indices, knn_dists, _ = nearest_neighbors(\n",
    "            X,\n",
    "            n_neighbors,\n",
    "            metric,\n",
    "            metric_kwds,\n",
    "            angular,\n",
    "            random_state,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "\n",
    "    knn_dists = knn_dists.astype(np.float32)\n",
    "\n",
    "    sigmas, rhos = smooth_knn_dist(\n",
    "        knn_dists,\n",
    "        float(n_neighbors),\n",
    "        local_connectivity=float(local_connectivity),\n",
    "    )\n",
    "\n",
    "    rows, cols, vals, dists = compute_membership_strengths(\n",
    "        knn_indices, knn_dists, sigmas, rhos, return_dists\n",
    "    )\n",
    "\n",
    "    result = scipy.sparse.coo_matrix(\n",
    "        (vals, (rows, cols)), shape=(X.shape[0], X.shape[0])\n",
    "    )\n",
    "    result.eliminate_zeros()\n",
    "\n",
    "    if apply_set_operations:\n",
    "        transpose = result.transpose()\n",
    "\n",
    "        prod_matrix = result.multiply(transpose)\n",
    "\n",
    "        result = (\n",
    "            set_op_mix_ratio * (result + transpose - prod_matrix)\n",
    "            + (1.0 - set_op_mix_ratio) * prod_matrix\n",
    "        )\n",
    "\n",
    "    result.eliminate_zeros()\n",
    "\n",
    "    if return_dists is None:\n",
    "        return result, sigmas, rhos\n",
    "    else:\n",
    "        if return_dists:\n",
    "            dmat = scipy.sparse.coo_matrix(\n",
    "                (dists, (rows, cols)), shape=(X.shape[0], X.shape[0])\n",
    "            )\n",
    "\n",
    "            dists = dmat.maximum(dmat.transpose()).todok()\n",
    "        else:\n",
    "            dists = None\n",
    "\n",
    "        return result, sigmas, rhos, dists  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build fuzzy_simplicial_set\n",
    "P, sigmas, rhos = fuzzy_simplicial_set(\n",
    "    X = X,\n",
    "    n_neighbors = new_n_neighbors,\n",
    "    metric = metric,\n",
    "    random_state = random_state,\n",
    "    knn_indices= new_knn_indices,\n",
    "    knn_dists = new_knn_dists,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Find the low dimensional representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap.umap_ import simplicial_set_embedding\n",
    "from umap.umap_ import dist\n",
    "#Dimensionality of the low dimensional representation\n",
    "n_components = 2\n",
    "\n",
    "negative_sample_rate = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings , aux_data = simplicial_set_embedding(\n",
    "    data = X,\n",
    "    graph = P,\n",
    "    n_components = n_components,\n",
    "    initial_alpha = 1.0,\n",
    "    a = a,\n",
    "    b = b,\n",
    "    gamma = 1.0,\n",
    "    negative_sample_rate = negative_sample_rate,\n",
    "    n_epochs = -1,\n",
    "    init = \"spectral\",\n",
    "    random_state = check_random_state(0),\n",
    "    metric = metric,\n",
    "    metric_kwds = {},\n",
    "    densmap = False,\n",
    "    densmap_kwds = {},\n",
    "    output_dens = False,\n",
    "    output_metric= dist.named_distances_with_gradients[\"euclidean\"],\n",
    "    output_metric_kwds={},\n",
    "    euclidean_output=True,\n",
    "    parallel=False,\n",
    "    verbose=True,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot for 2d\n",
    "colors = []\n",
    "colors += cm.get_cmap(\"Set3\").colors\n",
    "colors += cm.get_cmap(\"Set2\").colors\n",
    "my_cmap = ListedColormap(colors)\n",
    "#print(my_cmap)\n",
    "plt.rcParams.update(plt.rcParamsDefault)\n",
    "plt.scatter(embeddings[:, 0], embeddings[:, 1], c =y.astype(int),cmap=\"Spectral\",  s = .1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Metrics calculated \n",
    "\n",
    "\n",
    "from scipy.optimize import linear_sum_assignment as linear_assignment\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "\n",
    "def _make_cost_m(cm):\n",
    "    s = np.max(cm)\n",
    "    return (- cm + s)\n",
    "  \n",
    "def accurary_score(y_true, y_pred):\n",
    "  cm = metrics.confusion_matrix(y_true, y_pred)\n",
    "  indexes = linear_assignment(_make_cost_m(cm))\n",
    "  cm2 = cm[indexes[0], indexes[1]]\n",
    "  return np.sum(cm2) / np.sum(cm)\n",
    "  \n",
    "  \n",
    "\n",
    "def purity_score(y_true, y_pred):\n",
    "    # compute contingency matrix (also called confusion matrix)\n",
    "    contingency_matrix = metrics.cluster.contingency_matrix(y_true, y_pred)\n",
    "    # return purity\n",
    "    return np.sum(np.amax(contingency_matrix, axis=0)) / np.sum(contingency_matrix) \n",
    "\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(y, y_pred):\n",
    "    accuracy = [accurary_score(y, x) for x in  y_pred]\n",
    "    purity = [purity_score(y, x) for x in  y_pred]\n",
    "    adjusted_mutual_info_score = [metrics.adjusted_mutual_info_score(y, x) for x in  y_pred]\n",
    "    adjusted_rand_score = [metrics.adjusted_rand_score(y, x) for x in  y_pred]\n",
    "    normalized_mutual_info_score = [metrics.normalized_mutual_info_score(y, x) for x in  y_pred]\n",
    "    homogeneity_score = [metrics.homogeneity_score(y, x) for x in  y_pred]\n",
    "    completeness_score = [metrics.completeness_score(y, x) for x in  y_pred]\n",
    "    v_measure_score = [metrics.v_measure_score(y, x) for x in  y_pred]\n",
    "    \n",
    "\n",
    "    print(\"Accuracy Score: %0.3f\" % np.mean(accuracy))\n",
    "    print(\"Accuracy STD: %0.3f\" % np.std(accuracy))\n",
    "    print(\"Purity Score: %0.3f\" % np.mean(purity))\n",
    "    print(\"Purity STD: %0.3f\" % np.std(purity))\n",
    "      \n",
    "    print(\"Adjusted Mutual Information Score: %0.3f\" % np.mean(adjusted_mutual_info_score))\n",
    "    print(\"Adjusted Rand Index Score: %0.3f\" % np.mean(adjusted_rand_score))\n",
    "    print(\"Normalized Mutual Information Score: %0.3f\" % np.mean(normalized_mutual_info_score))\n",
    "    print(\"Normalized Mutual Information STD: %0.3f\" % np.std(normalized_mutual_info_score))\n",
    "    \n",
    "    print(\"Homogeneity: %0.3f\" % np.mean(homogeneity_score))\n",
    "    print(\"Completeness: %0.3f\" % np.mean(completeness_score))\n",
    "    print(\"V-measure: %0.3f\" % np.mean(v_measure_score))\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-Means clustering\n",
    "def kmeans(X, n_clusters, i):\n",
    "  kmeans = KMeans(n_clusters=n_clusters, random_state=i)\n",
    "  y_pred = kmeans.fit_predict(X)\n",
    "  return y_pred\n",
    "\n",
    "def agglo(X, n_clusters):\n",
    "  agglo = AgglomerativeClustering(n_clusters=n_clusters, linkage=\"ward\") #KMeans(n_clusters=n_clusters, random_state=i)\n",
    "  y_pred = agglo.fit_predict(X)\n",
    "  return y_pred_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#Bootstrap function to do hyperparam search\n",
    "def full_umap(X, metric, min_dist, dim, init_nn, to_add_nn, connectivity, rand_state, y):\n",
    "  \n",
    "  random_state = check_random_state(rand_state)\n",
    "  \n",
    "  a, b = find_ab_params(1.0, min_dist)\n",
    "  \n",
    "  knn_indices, knn_dists, knn_search_index = nearest_neighbors(\n",
    "    X,\n",
    "    n_neighbors=init_nn,\n",
    "    metric = metric,\n",
    "    metric_kwds = {},\n",
    "    angular=False,\n",
    "    random_state = random_state,\n",
    "    low_memory=True,\n",
    "    use_pynndescent=True,\n",
    "    n_jobs=1,\n",
    "    verbose=False,\n",
    "  )\n",
    "  \n",
    "  \n",
    "  if connectivity == \"org\":\n",
    "    new_knn_indices, new_knn_dists = knn_indices, knn_dists\n",
    "  else :\n",
    "    connected_mnn, mutual_nn, new_knn_indices, new_knn_dists  = mutual_nn_nearest(knn_indices, knn_dists, init_nn, to_add_nn, connectivity=connectivity)\n",
    "  \n",
    "  \n",
    "  \n",
    "  P, sigmas, rhos = fuzzy_simplicial_set(\n",
    "    X = X,\n",
    "    n_neighbors = to_add_nn,\n",
    "    metric = metric,\n",
    "    random_state = random_state,\n",
    "    knn_indices= new_knn_indices,\n",
    "    knn_dists = new_knn_dists,\n",
    "  )\n",
    "\n",
    "  embeddings, aux_data = simplicial_set_embedding(\n",
    "    data = X,\n",
    "    graph = P,\n",
    "    n_components = dim,\n",
    "    initial_alpha = 1,\n",
    "    a = a,\n",
    "    b = b,\n",
    "    gamma = 1.0,\n",
    "    negative_sample_rate = 5,\n",
    "    n_epochs = 200,\n",
    "    init = \"spectral\",\n",
    "    random_state = check_random_state(rand_state),\n",
    "    metric = metric,\n",
    "    metric_kwds = {},\n",
    "    densmap = False,\n",
    "    densmap_kwds = {},\n",
    "    output_dens = False,\n",
    "    output_metric= dist.named_distances_with_gradients[\"euclidean\"],\n",
    "    output_metric_kwds={},\n",
    "    euclidean_output=True,\n",
    "    parallel=False,\n",
    "    verbose=False,\n",
    "  )\n",
    "  embeddings = np.nan_to_num(embeddings)\n",
    "  return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cv_score(g, X, y, n_clusters, dim):\n",
    "  avg_score = []\n",
    "  for i in range(5):\n",
    "    X_r  = full_umap(X, g['metric'], g['min_dist'], dim, g['n_neighbors'], g['n_neighbors'],  g['connectivity'], i, y)\n",
    "    y_pred = kmeans(X_r, n_clusters, i)\n",
    "    avg_score.append(metrics.normalized_mutual_info_score(y, y_pred))\n",
    "  return (np.mean(avg_score), g)\n",
    "\n",
    "\n",
    "  \n",
    "def dimension_reduce(dimensions, X, y, n_clusters):\n",
    "  param_grid = [{\n",
    "        \"n_neighbors\": [10, 15, 20, 25, 30, 35, 50],\n",
    "        \"connectivity\":[ \"nearest\", \"min_tree\", \"full_tree\"],\n",
    "        \"metric\": ['cosine'],\n",
    "        \"min_dist\": np.linspace(0, 1, 11)\n",
    "    }]\n",
    "  param_grid = ParameterGrid(param_grid)\n",
    "  \n",
    "  \n",
    "  for dim in dimensions:\n",
    "    print(\"Reduce\")\n",
    "    results = []\n",
    "    results = np.array(Parallel(n_jobs=12)(delayed(cv_score)(g, X, y, n_clusters, dim)  for g in tqdm(param_grid))) \n",
    "    best_param = sorted(results, key=lambda t: t[0], reverse=True)[:1]\n",
    "    best_param = best_param[0][1] \n",
    "    print(best_param)\n",
    "    y_pred_train = []\n",
    "    for i in range(5):\n",
    "      X_umap = full_umap(X, best_param['metric'], best_param['min_dist'], dim, best_param['n_neighbors'], best_param['n_neighbors'], best_param['connectivity'], i, y)\n",
    "      y_pred = kmeans(X_umap, n_clusters, i)\n",
    "      y_pred_train.append(y_pred)\n",
    "    print(\"KMeans dimension reduce: \" + str(dim))\n",
    "    evaluate(y,y_pred_train)\n",
    "\n",
    "    \n",
    "    print(\"Reduce Nearest\")\n",
    "    results_reduced = []\n",
    "    for r in results:\n",
    "      if r[1][\"connectivity\"] == \"nearest\":\n",
    "        results_reduced.append(r)\n",
    "        \n",
    "    results_reduced = np.array(results_reduced)\n",
    "    \n",
    "    best_param = sorted(results_reduced, key=lambda t: t[0], reverse=True)[:1]\n",
    "    best_param = best_param[0][1] \n",
    "    print(best_param)\n",
    "    y_pred_train = []\n",
    "    for i in range(5):\n",
    "      X_umap = full_umap(X, best_param['metric'], best_param['min_dist'], dim, best_param['n_neighbors'], best_param['n_neighbors'], best_param['connectivity'], i, y)\n",
    "      y_pred = kmeans(X_umap, n_clusters, i)\n",
    "      y_pred_train.append(y_pred)\n",
    "    print(\"KMeans dimension reduce: \" + str(dim))\n",
    "    evaluate(y,y_pred_train)\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"Reduce Full Tree\")\n",
    "    results_reduced = []\n",
    "    for r in results:\n",
    "      if r[1][\"connectivity\"] == \"full_tree\":\n",
    "        results_reduced.append(r)\n",
    "        \n",
    "    results_reduced = np.array(results_reduced)\n",
    "    \n",
    "    best_param = sorted(results_reduced, key=lambda t: t[0], reverse=True)[:1]\n",
    "    best_param = best_param[0][1] \n",
    "    print(best_param)\n",
    "    y_pred_train = []\n",
    "    for i in range(5):\n",
    "      X_umap = full_umap(X, best_param['metric'], best_param['min_dist'], dim, best_param['n_neighbors'], best_param['n_neighbors'], best_param['connectivity'], i, y)\n",
    "      y_pred = kmeans(X_umap, n_clusters, i)\n",
    "      y_pred_train.append(y_pred)\n",
    "    print(\"KMeans dimension reduce: \" + str(dim))\n",
    "    evaluate(y,y_pred_train)\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"Reduce Min Tree\")\n",
    "    results_reduced = []\n",
    "    for r in results:\n",
    "      if r[1][\"connectivity\"] == \"min_tree\":\n",
    "        results_reduced.append(r)\n",
    "        \n",
    "    results_reduced = np.array(results_reduced)\n",
    "    \n",
    "    best_param = sorted(results_reduced, key=lambda t: t[0], reverse=True)[:1]\n",
    "    best_param = best_param[0][1] \n",
    "    print(best_param)\n",
    "    y_pred_train = []\n",
    "    for i in range(5):\n",
    "      X_umap = full_umap(X, best_param['metric'], best_param['min_dist'], dim, best_param['n_neighbors'], best_param['n_neighbors'], best_param['connectivity'], i, y)\n",
    "      y_pred = kmeans(X_umap, n_clusters, i)\n",
    "      y_pred_train.append(y_pred)\n",
    "    print(\"KMeans dimension reduce: \" + str(dim))\n",
    "    evaluate(y,y_pred_train)\n",
    "    \n",
    "    \n",
    "\n",
    "  \n",
    "      \n",
    "      \n",
    "dimension_reduce([2, 64], X, y, 20)      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
